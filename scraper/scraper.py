import os
import time
import csv
import glob
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from dotenv import load_dotenv

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service

from pymongo import MongoClient

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# ========================
# è¨­å®š
# ========================
BASE_URL = "https://www.maff.go.jp"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
}

CATEGORY_LIST_PAGES = [
    "/j/keikaku/syokubunka/k_ryouri/search_menu/type/rice.html",
    "/j/keikaku/syokubunka/k_ryouri/search_menu/type/noodles.html",
    "/j/keikaku/syokubunka/k_ryouri/search_menu/type/soup.html",
    "/j/keikaku/syokubunka/k_ryouri/search_menu/type/meat_vegetable.html",
    "/j/keikaku/syokubunka/k_ryouri/search_menu/type/fish.html",
]
CATEGORY_LIST_PAGES = [urljoin(BASE_URL, u) for u in CATEGORY_LIST_PAGES]

# ========================
# MongoDBè¨­å®š
# ========================
# MONGODB_URIã‚’å„ªå…ˆã€ãªã‘ã‚Œã°MONGO_URIã‚’è©¦è¡Œï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
MONGO_URI = os.getenv("MONGODB_URI") or os.getenv("MONGO_URI")
if not MONGO_URI:
    raise ValueError(
        "MONGODB_URIç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
        "Herokuç’°å¢ƒå¤‰æ•°ã¾ãŸã¯.envãƒ•ã‚¡ã‚¤ãƒ«ã§MONGODB_URIã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    )
DB_NAME = os.getenv("DB_NAME", "recipe")
COLLECTION_NAME = os.getenv("COLLECTION_NAME", "recipes")


# ========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================
def get_soup(url: str) -> BeautifulSoup:
    """requestsã§HTMLã‚’å–å¾—ã—ã¦BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™"""
    print(f"[GET] {url}")
    res = requests.get(url, headers=HEADERS, timeout=15)
    res.encoding = res.apparent_encoding
    res.raise_for_status()
    return BeautifulSoup(res.text, "html.parser")


def parse_ingredients(soup: BeautifulSoup):
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰ææ–™ + åˆ†é‡ã‚’ãƒ‘ãƒ¼ã‚¹
    æ§‹é€ ä¾‹:
    <ul class="menu_material clm2 mt10">
      <li>
        <ul class="list">
          <li>ç±³</li>
          <li>450gï¼ˆ3åˆï¼‰</li>
        </ul>
      </li>
      ...
    """
    result = []
    ul = soup.select_one("ul.menu_material")
    if not ul:
        return result

    # å„ææ–™ãƒ–ãƒ­ãƒƒã‚¯(li)
    for outer in ul.find_all("li", recursive=False):
        inner = outer.find("ul", class_="list")
        if not inner:
            continue
        lis = inner.find_all("li")
        if len(lis) >= 2:
            name = lis[0].get_text(strip=True)
            amount = lis[1].get_text(strip=True)
            if name or amount:
                result.append({"name": name, "amount": amount})
    return result


def parse_cooking_method(soup: BeautifulSoup) -> str:
    """
    ä½œã‚Šæ–¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ãƒ‘ãƒ¼ã‚¹
    æ§‹é€ ä¾‹:
    <h2 class="tit05 mt50">ä½œã‚Šæ–¹</h2>
    <ul class="recipe mt10">
      <li>
        <div class="num">1</div>
        <div class="txt">æ´»ãã®è‰¯ã„ã‚µãƒ¯ãƒ©ã‚’3æšã«ãŠã‚ã—ã€ä¸­éª¨ã«ãã£ã¦åŒ…ä¸ã‚’å…¥ã‚Œç¯€ã«ã¨ã‚‹ã€‚</div>
      </li>
      <li>
        <div class="num">2</div>
        <div class="txt">åˆºèº«ã«åˆ‡ã£ã¦åˆºèº«ã®3ï¼…ã®å¡©ã‚’ã‚ã¦20åˆ†ä½ãŠã„ãŸå¾Œã€ã•ã£ã¨æ´—ã„æ°´æ°—ã‚’å–ã‚Šã€é…¢ã«1æ™‚é–“ãã‚‰ã„ã¤ã‘ã‚‹ã€‚èº«ã®ä¸­ã¾ã§ç™½ããªã‚‹ã»ã©é…¢ã§ã—ã‚ã‚‹ã“ã¨ã€‚</div>
      </li>
      ...
    </ul>
    """
    # "ä½œã‚Šæ–¹"ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¢ã™
    h2 = soup.find("h2", class_="tit05", string=lambda text: text and "ä½œã‚Šæ–¹" in text)
    if not h2:
        # ä»–ã®å½¢å¼ã‚‚è©¦è¡Œ: h2å†…ã«"ä½œã‚Šæ–¹"ãƒ†ã‚­ã‚¹ãƒˆãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
        for h in soup.find_all("h2", class_="tit05"):
            if h.get_text(strip=True) and "ä½œã‚Šæ–¹" in h.get_text():
                h2 = h
                break
    
    if not h2:
        return ""
    
    # æ¬¡ã®å…„å¼Ÿè¦ç´ ã‹ã‚‰ul.recipeã‚’æ¢ã™
    recipe_ul = None
    for sibling in h2.find_next_siblings():
        if sibling.name == "ul" and "recipe" in sibling.get("class", []):
            recipe_ul = sibling
            break
        # ä»–ã®h2ã‚„ã‚¿ã‚¤ãƒˆãƒ«ãŒå‡ºãŸã‚‰ä¸­æ–­
        if sibling.name in ["h2", "h3", "h4"]:
            break
    
    if not recipe_ul:
        return ""
    
    # å„ã‚¹ãƒ†ãƒƒãƒ—(li)ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
    steps = []
    for li in recipe_ul.find_all("li", recursive=False):
        # numã¨txt divã‚’æ¢ã™
        txt_div = li.find("div", class_="txt")
        if txt_div:
            step_text = txt_div.get_text(strip=True)
            if step_text:
                steps.append(step_text)
    
    # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ç•ªå·ã‚’ä»˜ã‘ã¦è¿”ã™
    if steps:
        return "\n".join([f"{i+1}. {step}" for i, step in enumerate(steps)])
    
    return ""


def ingredients_to_string(ing_list):
    """
    DBã«ä¿å­˜ã•ã‚ŒãŸingredientsãƒªã‚¹ãƒˆã‚’
    CSV/Excelç”¨æ–‡å­—åˆ—ã«å¤‰æ›
    ä¾‹: ç±³ï¼š450gï¼ˆ3åˆï¼‰\næ°´ï¼š630ml ...
    """
    if not ing_list:
        return ""
    lines = []
    for item in ing_list:
        name = (item.get("name") or "").strip()
        amount = (item.get("amount") or "").strip()
        if name and amount:
            lines.append(f"{name}ï¼š{amount}")
        elif name:
            lines.append(name)
    return "\n".join(lines)


def get_section_clean(soup: BeautifulSoup, keyword: str) -> str:
    """
    'ä¸»ãªä½¿ç”¨é£Ÿæ', 'é£²é£Ÿæ–¹æ³•'ã®ã‚ˆã†ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã€‚
    æ­´å²/ç”±æ¥/æ™‚å­£/ä¿å­˜ãªã©ä»–ã®èª¬æ˜ãŒæ··ã–ã£ã¦å…¥ã£ã¦ãã‚‹ã®ã‚’
    ä¸€éƒ¨åˆ‡ã‚Šå–ã‚‹ãŸã‚ã®ç°¡å˜ãªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚‚å«ã‚€ã€‚
    """
    header = None

    # ã¾ãšh3ã‹ã‚‰æ¢ã™
    for h in soup.find_all("h3"):
        if keyword in h.get_text():
            header = h
            break

    # ãªã‘ã‚Œã°h4ã§ã‚‚è©¦è¡Œ
    if not header:
        for h in soup.find_all("h4"):
            if keyword in h.get_text():
                header = h
                break

    if not header:
        return ""

    texts = []

    # æ§‹é€ ãŒ<li><h3>ã‚¿ã‚¤ãƒˆãƒ«</h3> ... </li>ã®å ´åˆãŒå¤šã„ã®ã§ã€ã¾ãšparent liåŸºæº–ã§å–å¾—
    li = header.find_parent("li")
    if li:
        # è©²å½“liå†…ã§p / ul / olã®ã¿ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä½¿ç”¨
        for t in li.find_all(["p", "ul", "ol"], recursive=False):
            texts.append(t.get_text(" ", strip=True))
    else:
        # ã‚‚ã—ã‹ã—ãŸã‚‰liã§ãªã„å ´åˆã¯æ¬¡ã®å…„å¼Ÿè¦ç´ ã‹ã‚‰p / ul / olã‚’åé›†
        for sib in header.find_next_siblings():
            if sib.name in ["h3", "h4"]:
                break
            if sib.name in ["p", "ul", "ol"]:
                texts.append(sib.get_text(" ", strip=True))

    text = "\n".join(texts).strip()

    # ä¸è¦ãªé•·ã„èª¬æ˜ã‚’åˆ‡ã‚Šå–ã‚‹ï¼ˆç°¡å˜ãªé˜²å¾¡ï¼‰
    for cut in ["æ­´å²", "ç”±æ¥", "æ™‚å­£", "é–¢é€£", "ä¿å­˜", "ç¶™æ‰¿", "å–çµ„"]:
        idx = text.find(cut)
        if idx != -1:
            text = text[:idx].strip()

    return text


def collect_top5_from_category(cat_url: str, refresh_max: int = 20):
    """
    ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã§ãƒ©ãƒ³ãƒ€ãƒ ã«è¡¨ç¤ºã•ã‚Œã‚‹ãƒ¬ã‚·ãƒ”ã‚’ä½•åº¦ã‚‚ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã—ã¦
    æœ€å¤§5å€‹ã¾ã§è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’åé›†
    """
    print(f"\nğŸ”¹ ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒªã‚¹ãƒˆåé›†: {cat_url}")

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-software-rasterizer")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-background-networking")
    options.add_argument("--disable-background-timer-throttling")
    options.add_argument("--disable-backgrounding-occluded-windows")
    options.add_argument("--disable-breakpad")
    options.add_argument("--disable-client-side-phishing-detection")
    options.add_argument("--disable-default-apps")
    options.add_argument("--disable-features=TranslateUI")
    options.add_argument("--disable-hang-monitor")
    options.add_argument("--disable-ipc-flooding-protection")
    options.add_argument("--disable-popup-blocking")
    options.add_argument("--disable-prompt-on-repost")
    options.add_argument("--disable-renderer-backgrounding")
    options.add_argument("--disable-sync")
    options.add_argument("--disable-translate")
    options.add_argument("--metrics-recording-only")
    options.add_argument("--no-first-run")
    options.add_argument("--safebrowsing-disable-auto-update")
    options.add_argument("--enable-automation")
    options.add_argument("--password-store=basic")
    options.add_argument("--use-mock-keychain")
    options.add_argument("--window-size=1920,1080")
    
    # Herokuç’°å¢ƒã§ã®ChromeDriverãƒ‘ã‚¹è¨­å®š
    # heroku-buildpack-chrome-for-testingãŒè¨­å®šã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‚’å„ªå…ˆ
    chrome_binary = os.getenv("GOOGLE_CHROME_BIN") or os.getenv("CHROME_BIN")
    chromedriver_path = os.getenv("CHROMEDRIVER_PATH") or os.getenv("CHROMEDRIVER_BIN")
    
    # Heroku buildpackãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸChromeã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹
    # chrome-for-testing buildpackã®ãƒ‘ã‚¹ã‚’å„ªå…ˆ
    if not chrome_binary:
        default_chrome_paths = [
            "/app/.chrome-for-testing/chrome-linux64/chrome",  # chrome-for-testing (ç›´æ¥ãƒ‘ã‚¹)
            "/app/.chrome-for-testing/chrome/linux-*/chrome-linux64/chrome",  # chrome-for-testing (glob)
            "/app/.chromedriver/bin/google-chrome",  # æ—§buildpack
            "/usr/bin/google-chrome",
        ]
        for path_pattern in default_chrome_paths:
            if "*" in path_pattern:
                # globãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
                matches = glob.glob(path_pattern)
                if matches:
                    chrome_binary = matches[0]
                    break
            elif os.path.exists(path_pattern):
                chrome_binary = path_pattern
                break
    
    if chrome_binary:
        options.binary_location = chrome_binary
        print(f"  ğŸ”§ Chrome binary: {chrome_binary}")
    
    # Herokuç’°å¢ƒã§ã®ChromeDriver Serviceè¨­å®š
    # chrome-for-testing buildpackã®ãƒ‘ã‚¹ã‚’å„ªå…ˆ
    if not chromedriver_path:
        default_chromedriver_paths = [
            "/app/.chrome-for-testing/chromedriver-linux64/chromedriver",  # chrome-for-testing (ç›´æ¥ãƒ‘ã‚¹)
            "/app/.chrome-for-testing/chromedriver/linux-*/chromedriver-linux64/chromedriver",  # chrome-for-testing (glob)
            "/app/.chromedriver/bin/chromedriver",  # æ—§buildpack
            "/usr/local/bin/chromedriver",
            "/app/vendor/chromedriver/bin/chromedriver",
        ]
        for path_pattern in default_chromedriver_paths:
            if "*" in path_pattern:
                # globãƒ‘ã‚¿ãƒ¼ãƒ³ã®å ´åˆ
                matches = glob.glob(path_pattern)
                if matches:
                    chromedriver_path = matches[0]
                    print(f"  ğŸ” Found ChromeDriver at: {chromedriver_path}")
                    break
            elif os.path.exists(path_pattern):
                chromedriver_path = path_pattern
                print(f"  ğŸ” Found ChromeDriver at: {chromedriver_path}")
                break
    
    if chromedriver_path and os.path.exists(chromedriver_path):
        service = Service(chromedriver_path)
        print(f"  ğŸ”§ Using ChromeDriver: {chromedriver_path}")
        try:
            driver = webdriver.Chrome(service=service, options=options)
        except Exception as e:
            print(f"  âŒ Failed to start Chrome with Service: {e}")
            print("  âš ï¸  Falling back to default ChromeDriver")
            driver = webdriver.Chrome(options=options)
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ChromeDriverã‚’ä½¿ç”¨
        print("  âš ï¸  ChromeDriver path not found, using default")
        print("  ğŸ’¡ Make sure Chrome buildpacks are added to Heroku")
        try:
            driver = webdriver.Chrome(options=options)
        except Exception as e:
            print(f"  âŒ Failed to start Chrome: {e}")
            raise

    urls = set()
    no_new = 0

    try:
        for i in range(refresh_max):
            driver.get(cat_url)
            time.sleep(2)

            sections = driver.find_elements(By.CSS_SELECTOR, "div[id^='SearchMenu']")
            if not sections:
                print("  [WARN] SearchMenuã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                break

            sec_id = sections[0].get_attribute("id")

            cards = driver.find_elements(
                By.CSS_SELECTOR, f"div#{sec_id} div.list p.tit a[href]"
            )

            before = len(urls)
            for c in cards:
                href = c.get_attribute("href")
                if href:
                    urls.add(href)

            added = len(urls) - before
            print(f"  â†º {i+1}å›ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥: +{added}å€‹ï¼ˆç´¯è¨ˆ {len(urls)}å€‹ï¼‰")

            if added == 0:
                no_new += 1
            else:
                no_new = 0

            if no_new >= 3:
                print("  âœ… æ–°ã—ã„URLãŒã‚‚ã†ãªã„ãŸã‚çµ‚äº†")
                break

            if len(urls) >= 5:
                print("  âœ… 5å€‹ä»¥ä¸Šåé›†å®Œäº†")
                break

    finally:
        driver.quit()

    urls = list(urls)[:5]
    print(f"  â¤ æœ€çµ‚é¸æŠã•ã‚ŒãŸURL {len(urls)}å€‹")
    return urls


def scrape_detail_page(url: str) -> dict:
    """
    è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º:
    - title (æ–™ç†å)
    - main_image (ä»£è¡¨ç”»åƒURL)
    - main_ingredients (ä¸»ãªä½¿ç”¨é£Ÿæã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ)
    - eating_method (é£²é£Ÿæ–¹æ³•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ)
    - cooking_method (ä½œã‚Šæ–¹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ) - æ–°è¦è¿½åŠ 
    - ingredients (ææ–™ + åˆ†é‡ãƒªã‚¹ãƒˆ)
    - detailUrl (ãƒšãƒ¼ã‚¸URL)
    """
    print(f"[GET è©³ç´°] {url}")
    soup = get_soup(url)

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_span = soup.select_one("span.name")
    title = title_span.get_text(strip=True) if title_span else ""

    # ãƒ¡ã‚¤ãƒ³ç”»åƒ
    img_tag = soup.select_one("div.menu_main img.resp_img")
    if img_tag and img_tag.get("src"):
        main_image = urljoin(url, img_tag["src"])
    else:
        main_image = ""

    # ä¸»ãªä½¿ç”¨é£Ÿæ / é£²é£Ÿæ–¹æ³•
    main_ingredients = get_section_clean(soup, "ä¸»ãªä½¿ç”¨é£Ÿæ")
    eating_method = get_section_clean(soup, "é£²é£Ÿæ–¹æ³•")

    # ä½œã‚Šæ–¹
    cooking_method = parse_cooking_method(soup)

    # ææ–™ + åˆ†é‡
    ingredients = parse_ingredients(soup)

    return {
        "title": title,
        "main_image": main_image,
        "main_ingredients": main_ingredients,
        "eating_method": eating_method,
        "cooking_method": cooking_method,  # æ–°è¦è¿½åŠ 
        "ingredients": ingredients,  # DBã«é…åˆ—ã¨ã—ã¦ä¿å­˜
        "detailUrl": url,
    }


# ========================
# DBä¿å­˜
# ========================
def save_to_mongo(rows):
    """
    MongoDBã«upsertã§ä¿å­˜
    - key: detailUrl
    - é‡è¤‡ã®å ´åˆã¯scrapeCountå¢—åŠ  + ãƒ‡ãƒ¼ã‚¿æ›´æ–°
    """
    if not rows:
        return

    client = MongoClient(
        MONGO_URI,
        serverSelectionTimeoutMS=30000,  # 30ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        connectTimeoutMS=30000,
    )
    db = client[DB_NAME]
    col = db[COLLECTION_NAME]

    count = 0
    for doc in rows:
        col.update_one(
            {"detailUrl": doc["detailUrl"]},
            {
                "$set": doc,
                "$inc": {"scrapeCount": 1},
                "$setOnInsert": {"createdAt": time.time()},
            },
            upsert=True,
        )
        count += 1

    client.close()
    print(f"ğŸ’¾ MongoDBä¿å­˜/æ›´æ–°å®Œäº†: {count}ä»¶")


# ========================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ========================
def check_existing_recipes(urls: list) -> set:
    """
    MongoDBã«æ—¢ã«å­˜åœ¨ã™ã‚‹ãƒ¬ã‚·ãƒ”ã®URLã‚’ç¢ºèª
    è¿”ã‚Šå€¤: æ—¢å­˜ã®URLã®ã‚»ãƒƒãƒˆ
    """
    if not urls:
        return set()
    
    try:
        client = MongoClient(
            MONGO_URI,
            serverSelectionTimeoutMS=10000,  # 10ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            connectTimeoutMS=10000,
        )
        db = client[DB_NAME]
        col = db[COLLECTION_NAME]
        
        existing = col.find(
            {"detailUrl": {"$in": urls}},
            {"detailUrl": 1}
        )
        existing_urls = {doc["detailUrl"] for doc in existing}
        
        client.close()
        return existing_urls
    except Exception as e:
        print(f"âš ï¸  æ—¢å­˜ãƒ¬ã‚·ãƒ”ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        return set()


def main():
    all_rows = []
    total_new_count = 0
    total_existing_count = 0
    no_new_data_categories = 0

    print("=" * 60)
    print("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹")
    print("=" * 60)

    for cat_url in CATEGORY_LIST_PAGES:
        category_name = cat_url.split("/")[-1].replace(".html", "")  # rice, soupãªã©
        print(f"\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªãƒ¼: {category_name}")
        
        # ã‚«ãƒ†ã‚´ãƒªãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰URLã‚’åé›†
        links = collect_top5_from_category(cat_url, refresh_max=20)
        
        if not links:
            print(f"  âš ï¸  {category_name}ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            no_new_data_categories += 1
            continue

        # æ—¢å­˜ã®ãƒ¬ã‚·ãƒ”ã‚’ç¢ºèª
        existing_urls = check_existing_recipes(links)
        new_links = [link for link in links if link not in existing_urls]
        
        print(f"  ğŸ“Š åé›†URL: {len(links)}å€‹")
        print(f"  âœ… æ–°è¦URL: {len(new_links)}å€‹")
        print(f"  ğŸ”„ æ—¢å­˜URL: {len(existing_urls)}å€‹")

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
        if not new_links:
            print(f"  â¸ï¸  {category_name}ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã¯æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            no_new_data_categories += 1
            total_existing_count += len(existing_urls)
            
            # ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€æ—©æœŸçµ‚äº†
            if no_new_data_categories >= len(CATEGORY_LIST_PAGES):
                print("\n" + "=" * 60)
                print("â¹ï¸  ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                print("   ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                print("=" * 60)
                return
            continue

        # æ–°ã—ã„ãƒ¬ã‚·ãƒ”ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        category_new_count = 0
        for link in new_links:
            try:
                data = scrape_detail_page(link)
                data["category"] = category_name
                all_rows.append(data)
                category_new_count += 1
                total_new_count += 1
                time.sleep(1)
            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {link} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«å¤±æ•—: {e}")
                continue

        total_existing_count += len(existing_urls)
        print(f"  âœ… {category_name}ã‚«ãƒ†ã‚´ãƒªãƒ¼: æ–°è¦ {category_new_count}ä»¶ã‚’è¿½åŠ ")

    # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
    if not all_rows:
        print("\n" + "=" * 60)
        print("â¹ï¸  æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        print(f"   æ—¢å­˜ãƒ¬ã‚·ãƒ”: {total_existing_count}ä»¶")
        print("   ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        print("=" * 60)
        return

    # DBä¿å­˜
    print(f"\nğŸ”¥ åˆè¨ˆ {len(all_rows)}ä»¶ã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’DBã«upsert")
    save_to_mongo(all_rows)

    # CSVãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”Ÿæˆ
    file = "maff_recipe_top5_each_category.csv"
    keys = [
        "title",
        "main_image",
        "main_ingredients",
        "eating_method",
        "cooking_method",  # æ–°è¦è¿½åŠ 
        "ingredients",   # ææ–™+åˆ†é‡ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦å…¥ã‚Œã‚‹
        "detailUrl",
        "category",
    ]

    with open(file, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        for row in all_rows:
            out = {k: row.get(k, "") for k in keys}
            # ingredientsã¯ãƒªã‚¹ãƒˆ â†’ æ–‡å­—åˆ—ã«å¤‰æ›
            out["ingredients"] = ingredients_to_string(row.get("ingredients", []))
            writer.writerow(out)

    print(f"\nâœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° + DBä¿å­˜ + CSVãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†!")
    print(f"   â†’ æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(all_rows)}ä»¶")
    print(f"   â†’ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {total_existing_count}ä»¶")
    print(f"   â†’ CSVãƒ•ã‚¡ã‚¤ãƒ«: {file}")
    print("=" * 60)


if __name__ == "__main__":
    main()
